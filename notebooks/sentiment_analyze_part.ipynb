{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8637a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import grangercausalitytests, adfuller\n",
    "from statsmodels.tsa.api import VAR\n",
    "from scipy.stats import normaltest\n",
    "import os\n",
    "import pickle\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3d1468b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finbert sentiment data\n",
    "df = pd.read_pickle('final_m.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2290fbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"whole process for data filtration and strategy\"\"\"\n",
    "# # if choose other data\n",
    "# # set path\n",
    "# directory = 'the path'\n",
    "# # get all Parquet file in this path\n",
    "def parquet_concat(directory):\n",
    "    \"\"\"\n",
    "    Aggregates raw sentiment data from multiple parquet files\n",
    "    \n",
    "    Parameters:\n",
    "        directory (str): Path containing parquet files\n",
    "        \n",
    "    Process Flow:\n",
    "        1. Load all parquet files in target directory\n",
    "        2. Merge with mapping table for data validation\n",
    "        3. Filter by publication date (post-2017-08-17)\n",
    "        4. Apply logarithmic transformation to FAMC values\n",
    "        5. Explicit garbage collection for memory management\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: Cleaned, merged dataset with essential features\n",
    "    \"\"\"\n",
    "    parquet_files = [f for f in os.listdir(directory) if f.endswith('.parquet')]\n",
    "    # read and merge all the Parquet files\n",
    "    df_list = []\n",
    "    for file in parquet_files:\n",
    "        file_path = os.path.join(directory, file)\n",
    "        temp_df = pd.read_parquet(file_path)\n",
    "        df_list.append(temp_df)\n",
    "\n",
    "    df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    # use map table to filter these sentiment data\n",
    "    map_table = pd.read_pickle('final_map_table.pkl')\n",
    "    df = pd.merge(df, map_table, on=['id'], how='right')   \n",
    "    df['date'] = [x[:10] for x in df['publishedDate']]\n",
    "    df = df[df['date'] >= '2017-08-17']\n",
    "    df['log_FAMC'] = np.log(df['FAMC'])\n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "\n",
    "def sentiment_cluster(df_, stock_counts, fin_col, weighted_method, freq='d', impact=3):\n",
    "    \"\"\"\n",
    "    Generates market-level sentiment indicators with news impact weighting\n",
    "    \n",
    "    Parameters:\n",
    "        df_ (DataFrame): Filtered sentiment data from parquet_concat\n",
    "        stock_counts (Series): Market-wide news frequency baseline\n",
    "        fin_col (list): Sentiment score columns to aggregate\n",
    "        weighted_method (str): Weighting scheme ('ew', 'vw', or 'log vw')\n",
    "        freq (str): Time frequency ('d' daily/'h' hourly)\n",
    "        impact (float): News impact multiplier (default 3x)\n",
    "    \n",
    "    Key Processes:\n",
    "        1. Intra-period deduplication: Average scores per ticker-period\n",
    "        2. Weighted aggregation: News impact-adjusted averages\n",
    "        3. Sentiment persistence: Forward filling for assets without news\n",
    "        4. Adaptive smoothing: Blends current news impact with historical values\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: Time-series of market sentiment indicators\n",
    "    \"\"\"\n",
    "    \"\"\"ps: 1. some stocks have many news during same period, we should avoid double counting.\"\"\"\n",
    "    df_ = df_.copy()\n",
    "    if freq == 'h':\n",
    "        freq_col = 'publishedDate'\n",
    "        stock_counts.name = 'count'\n",
    "        df_ = pd.merge(df_, stock_counts, left_on='date', right_index=True, how='left')\n",
    "        stock_counts = df_[['count', freq_col]].drop_duplicates().set_index([freq_col])\n",
    "    else:\n",
    "        freq_col = 'date'\n",
    "        stock_counts.name = 'count'\n",
    "    # for the repeated news about the same stock at a certain period, we take the average.\n",
    "    df_['ew'] = 1\n",
    "    temp_senti_s = df_.groupby(['ticker', freq_col]).apply(lambda x: x[fin_col + [weighted_method]].mean(axis=0))\n",
    "    # suppose the stocks have no news remain the same sentiment score as last period.\n",
    "    temp_senti_freq = temp_senti_s.groupby(level=1).apply(lambda x: pd.DataFrame(np.average(\n",
    "        a=x[fin_col].values, weights=x[weighted_method].values, axis=0), index=fin_col).T).reset_index(level=1, drop=True)\n",
    "    temp_count_freq = temp_senti_s.groupby(level=1).apply(lambda x: x.shape[0])\n",
    "    temp_count_freq.name = 'inner_count'\n",
    "    temp_merge = pd.concat([stock_counts, temp_count_freq, temp_senti_freq], axis=1).sort_index().ffill()\n",
    "    cluster_dict = {x: [] for x in fin_col}\n",
    "    last_senti = {x: temp_merge.head(1)[x].values[0] for x in fin_col}\n",
    "    for ind, row in temp_merge.iterrows():\n",
    "        for s in fin_col:\n",
    "            temp_put = (row[s] * impact * row['inner_count'] + last_senti[s] * (row['count'] - row['inner_count'])\n",
    "                           ) / (impact * row['inner_count'] + row['count'] - row['inner_count'])\n",
    "            \n",
    "            cluster_dict[s].append(temp_put)\n",
    "            last_senti[s] = temp_put\n",
    "    return pd.DataFrame(data=cluster_dict, index=temp_merge.index)\n",
    "\n",
    "\n",
    "def sector_cluster(df_, stock_counts_sector, fin_col, weighted_method, freq='d', impact=3):\n",
    "    \"\"\"\n",
    "    Sector-specific version of sentiment_cluster\n",
    "    \n",
    "    Parameters:\n",
    "        stock_counts_sector (DataFrame): Sector-annotated news frequency data\n",
    "        \n",
    "    Process Flow:\n",
    "        1. Splits data by GICS sector classification\n",
    "        2. Parallel processing of sector groups\n",
    "        3. Aggregates sector-specific sentiment indicators\n",
    "        \n",
    "    Returns:\n",
    "        dict: {sector_code: DataFrame} of sector sentiment indicators\n",
    "    \"\"\"\n",
    "    sec_dict = {}\n",
    "    for sector, group in df_.groupby('gsector'):\n",
    "        tc = stock_counts_sector[stock_counts_sector.index.get_level_values(0) == sector].reset_index(level=0, drop=True)\n",
    "        temp_d = sentiment_cluster(group, tc, fin_col, weighted_method, freq, impact)\n",
    "        sec_dict[sector] = temp_d\n",
    "    return sec_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de7e5b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_manipulation_pipeline(config):\n",
    "    \"\"\"\n",
    "    Main execution pipeline for sentiment data processing and analysis\n",
    "    \n",
    "    Parameters:\n",
    "        config (dict): Configuration dictionary with keys:\n",
    "            - model: List of model names to process\n",
    "            - pkl_file: List of preprocessed data cache paths\n",
    "            - parquet_file: List of raw data directories\n",
    "            - fin_col: List of sentiment score columns per model\n",
    "            - impacts: List of impact factors per model\n",
    "    \n",
    "    Returns:\n",
    "        dict: Nested dictionary structure containing:\n",
    "            {\n",
    "                model_name: {\n",
    "                    weighting_scheme: [\n",
    "                        market_cluster_df,\n",
    "                        sector_cluster_dict\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "    \n",
    "    Key Features:\n",
    "        - Caching system avoids redundant data processing\n",
    "        - Parallel processing for multiple models\n",
    "        - Three weighting schemes (equal, volume, log-volume)\n",
    "        - Generates both market-wide and sector-specific results\n",
    "        - Maintains model separation throughout processing\n",
    "    \n",
    "    Workflow:\n",
    "        1. Data Loading Phase:\n",
    "           - Checks for existing preprocessed data (pickle files)\n",
    "           - Processes raw parquet files only when necessary\n",
    "        2. Count Data Integration:\n",
    "           - Loads precomputed news frequency statistics\n",
    "        3. Sentiment Aggregation:\n",
    "           - Computes market-level sentiment clusters\n",
    "           - Generates sector-specific sentiment clusters\n",
    "        4. Result Organization:\n",
    "           - Structures outputs by model and weighting scheme\n",
    "    \"\"\"\n",
    "    summary_dict = {}\n",
    "    for i, model_ in enumerate(config['model']):\n",
    "        # first step: load data\n",
    "        if os.path.exists(config['pkl_file'][i]):\n",
    "            print('file already ready')\n",
    "            df = pd.read_pickle(config['pkl_file'][i])\n",
    "        else:\n",
    "            df = parquet_concat(config['parquet_file'][i])\n",
    "            df.to_pickle(config['pkl_file'][i])\n",
    "        \n",
    "        # counts data\n",
    "        ticker_counts = pd.read_pickle('ticker_counts_daily.pkl')\n",
    "        ticker_counts_sector = pd.read_pickle('ticker_counts_sector_daily.pkl')\n",
    "        \n",
    "        # \n",
    "        contrast_dict = {}\n",
    "        for w in ['ew', 'FAMC', 'log_FAMC']:\n",
    "            contrast_dict[w] = []\n",
    "            contrast_dict[w].append(sentiment_cluster(df, ticker_counts, config['fin_col'][i], w, 'd', config['impacts'][i]))\n",
    "            contrast_dict[w].append(sector_cluster(df, ticker_counts_sector, config['fin_col'][i], w, 'd', config['impacts'][i]))\n",
    "        \n",
    "        summary_dict[model_] = contrast_dict\n",
    "    \n",
    "    return summary_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3154a062",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_ = {\n",
    "    'model': ['finbert', 'deberta', 'roberta'],\n",
    "    'pkl_file': ['finbert_senti.pkl', 'deberta_senti.pkl', 'roberta_senti.pkl'],\n",
    "    'parquet_file': [],  # 老师填一下，顺序是'finbert', 'deberta', 'roberta'\n",
    "    'fin_col': [['finbert_pos', 'finbert_neg']]*3,\n",
    "    'impacts': [3, 3, 3] \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785449d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_model_sentiment = data_manipulation_pipeline(config_)\n",
    "with open('sentiment_summary_dict.pkl', 'wb') as file:\n",
    "    pickle.dump(all_model_sentiment, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_3.9_torch",
   "language": "python",
   "name": "env_3.9_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
