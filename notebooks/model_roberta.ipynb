{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eec47f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "from statsmodels.tsa.api import VAR\n",
    "import os\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3669479b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f87dfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change dir\n",
    "if os.path.exists(\"../data\"):\n",
    "    os.chdir(\"../data\")\n",
    "else:\n",
    "    print(\"The directory ../data does not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d4be62c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "939"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"whole process for data filtration\"\"\"\n",
    "# if choose other data\n",
    "# set path\n",
    "directory = 'roberta'\n",
    "\n",
    "# get all Parquet file in this path\n",
    "parquet_files = [f for f in os.listdir(directory) if f.endswith('.parquet')]\n",
    "\n",
    "# read and merge all the Parquet files\n",
    "df_list = []\n",
    "for file in parquet_files:\n",
    "    file_path = os.path.join(directory, file)\n",
    "    temp_df = pd.read_parquet(file_path)\n",
    "    df_list.append(temp_df)\n",
    "\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# use map table to filter these sentiment data\n",
    "map_table = pd.read_pickle('final_map_table.pkl')\n",
    "df = pd.merge(df, map_table, on=['id'], how='right')\n",
    "# del combined_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "093a16df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['date'] = df['date'] = [x[:10] for x in df['publishedDate']]\n",
    "\n",
    "df = df[df['date'] >= '2017-08-17']\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e7346d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts data\n",
    "ticker_counts = pd.read_pickle('sNv.pkl').groupby('datadate').apply(lambda x: len(x['tic'].unique()))\n",
    "ticker_counts_sector = pd.read_pickle('sNv.pkl').groupby(['gsector', 'datadate']).apply(lambda x: len(x['tic'].unique()))\n",
    "ticker_counts = ticker_counts[(ticker_counts.index >= '2017-08-17') & (ticker_counts.index <= '2022-09-06')]\n",
    "ticker_counts_sector = ticker_counts_sector[(ticker_counts_sector.index.get_level_values(1) >= '2017-08-17'\n",
    "                                            ) & (ticker_counts_sector.index.get_level_values(1) <= '2022-09-06')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8d882f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_cluster(df_, stock_counts, analysis_on = 'finbert', freq='d', impact=3):\n",
    "    \"\"\"ps: 1. some stocks have many news during same period, we should avoid double counting.\n",
    "           2. \"\"\"\n",
    "    df_ = df_.copy()\n",
    "    if freq == 'h':\n",
    "        freq_col = 'publishedDate'\n",
    "        stock_counts.name = 'count'\n",
    "        df_ = pd.merge(df_, stock_counts, left_on='date', right_index=True, how='left')\n",
    "        stock_counts = df_[['count', freq_col]].drop_duplicates().set_index([freq_col])\n",
    "    else:\n",
    "        freq_col = 'date'\n",
    "        stock_counts.name = 'count'\n",
    "    \n",
    "    pos_col = analysis_on + '_pos'\n",
    "    neg_col = analysis_on + '_neg'\n",
    "    neu_col = analysis_on + '_neu'\n",
    "\n",
    "    # for the repeated news about the same stock at a certain period, we take the average.\n",
    "    temp_senti_s = df_.groupby(['ticker', freq_col]).apply(lambda x: x[[pos_col, neg_col, neu_col]].mean(axis=0))\n",
    "    # suppose the stocks have no news remain the same sentiment score as last period.\n",
    "    temp_senti_freq = temp_senti_s.groupby(level=1).mean()\n",
    "    temp_count_freq = temp_senti_s.groupby(level=1).apply(lambda x: x.shape[0])\n",
    "    temp_count_freq.name = 'inner_count'\n",
    "    temp_merge = pd.concat([stock_counts, temp_count_freq, temp_senti_freq], axis=1).sort_index().ffill()\n",
    "    cluster_dict = {pos_col: [], neg_col: [], neu_col: []}\n",
    "    last_senti = {pos_col: temp_merge.head(1)[pos_col].values[0],\n",
    "                  neg_col: temp_merge.head(1)[neg_col].values[0],\n",
    "                  neu_col: temp_merge.head(1)[neu_col].values[0]}\n",
    "    for ind, row in temp_merge.iterrows():\n",
    "        print(ind)\n",
    "        for s in [pos_col, neg_col, neu_col]:\n",
    "            temp_put = (row[s] * impact * row['inner_count'] + last_senti[s] * (row['count'] - row['inner_count'])\n",
    "                           ) / (impact * row['inner_count'] + row['count'] - row['inner_count'])\n",
    "            \n",
    "            cluster_dict[s].append(temp_put)\n",
    "            last_senti[s] = temp_put\n",
    "    return pd.DataFrame(data=cluster_dict, index=temp_merge.index)\n",
    "\n",
    "\n",
    "def sector_cluster(df_, stock_counts_sector):\n",
    "    sec_dict = {}\n",
    "    for sector, group in df_.groupby('gsector'):\n",
    "        tc = stock_counts_sector[stock_counts_sector.index.get_level_values(0) == sector].reset_index(level=0, drop=True)\n",
    "        temp_d = sentiment_cluster(group, tc, analysis_on='roberta', freq='d')\n",
    "        sec_dict[sector] = temp_d\n",
    "    return sec_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9039e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "see = sector_cluster(df, ticker_counts_sector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c02d8133",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_cluster = sentiment_cluster(df, ticker_counts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finbert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
